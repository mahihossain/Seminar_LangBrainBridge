{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, WhisperModel\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "# ------ Hyperparameters ------- #\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "alpha = 1  # Weight for text embeddings in concatenation\n",
    "beta = 1   # Weight for audio embeddings in concatenation\n",
    "\n",
    "# ------- Model Definition ------- #\n",
    "class M2BAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M2BAM, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.whisper = WhisperModel.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "        self.multitask_layer = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim + audio_embedding_dim, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_voxels_reading),  \n",
    "            nn.Linear(512, num_voxels_listening)  \n",
    "        )\n",
    "\n",
    "    def forward(self, text_input, audio_input):\n",
    "        text_embeddings = self.bert(**text_input).last_hidden_state[:, 0, :]  \n",
    "        audio_embeddings = self.whisper(**audio_input).last_hidden_state[:, 0, :]\n",
    "\n",
    "        combined_embeddings = torch.cat((text_embeddings, audio_embeddings), dim=1)  # Concatenate along feature dimension\n",
    "        predictions = self.multitask_layer(combined_embeddings)\n",
    "\n",
    "        reading_pred, listening_pred = predictions[:, :num_voxels_reading], predictions[:, num_voxels_reading:]\n",
    "        return reading_pred, listening_pred\n",
    "\n",
    "# ------- Data Loading Function  ------- #\n",
    "def load_data(data_dir, task):\n",
    "    df = pd.read_hdf(\"data/df_text.hdf\")  # Load the DataFrame\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), desc=\"Loading Data\", total=df.shape[0]):\n",
    "        story_name = row['story_name']\n",
    "        text = row['text']\n",
    "        task = row['task']\n",
    "\n",
    "        # Text Input Processing \n",
    "        text_input = bert_tokenizer(text, return_tensors='pt')  # Assuming you have a BERT tokenizer\n",
    "\n",
    "        if task == 'listening': \n",
    "            aligned_audio_file = row['aligned_audio_file']\n",
    "            audio_input = whisper_processor(aligned_audio_file, return_tensors='pt')  # Assuming a Whisper processor\n",
    "        else:\n",
    "            audio_input = None  # Placeholder for consistency\n",
    "\n",
    "        # fMRI Target Data (You'll need to fill this based on how you load fMRI voxels)\n",
    "        fmri_filename = eval(f\"fmri_{task}_{split}\") \n",
    "        fmri_file = fmri_filename.format(row['subject'])\n",
    "        with h5py.File(fmri_file, 'r') as f: \n",
    "            target_voxel_data = f[story_name][:]  # Example, adjust as needed\n",
    "\n",
    "        yield text_input, audio_input, target_voxel_data\n",
    "\n",
    "# ------- Training Loop ------- #\n",
    "model = M2BAM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()  # Example loss function\n",
    "\n",
    "# Load all data into memory\n",
    "all_data = list(load_data(data_dir, task))\n",
    "\n",
    "# Initialize KFold cross-validator\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(all_data)):\n",
    "    # Split data into training and validation sets\n",
    "    train_data = [all_data[i] for i in train_indices]\n",
    "    test_data = [all_data[i] for i in test_indices]\n",
    "\n",
    "    # Train on the training set\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (text_batch, audio_batch, voxel_batch) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            reading_output, listening_output = model(text_batch, audio_batch)\n",
    "\n",
    "            # Compute loss based on your target voxel data and choice of loss function\n",
    "            loss_reading = criterion(reading_output, voxel_batch[:, :num_voxels_reading]) \n",
    "            loss_listening = criterion(listening_output, voxel_batch[:, num_voxels_reading:])\n",
    "            loss = loss_reading + loss_listening\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            if i % 10 == 0:  # Log every 10 batches\n",
    "                logging.info(f'Fold: {fold}, Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}')\n",
    "\n",
    "        # Save the model and optimizer state after each epoch\n",
    "        torch.save({\n",
    "            'fold': fold,\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, f'm2bam_model_fold_{fold}_epoch_{epoch}.pt')\n",
    "        \n",
    "\n",
    "        # Evaluate on the test set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for i, (text_batch, audio_batch, voxel_batch) in enumerate(test_data):\n",
    "                reading_output, listening_output = model(text_batch, audio_batch)\n",
    "\n",
    "                # Compute loss based on your target voxel data and choice of loss function\n",
    "                loss_reading = criterion(reading_output, voxel_batch[:, :num_voxels_reading]) \n",
    "                loss_listening = criterion(listening_output, voxel_batch[:, num_voxels_reading:])\n",
    "                loss = loss_reading + loss_listening\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Calculate the average loss over the test set\n",
    "        average_loss = total_loss / len(test_data)\n",
    "\n",
    "        logging.info(f'Fold: {fold}, Test Loss: {average_loss}')\n",
    "        \n",
    "        \n",
    "        # Predict and save the results\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        predictions = []\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for i, (text_batch, audio_batch, voxel_batch) in enumerate(test_data):\n",
    "                reading_output, listening_output = model(text_batch, audio_batch)\n",
    "                predictions.append(reading_output)\n",
    "        \n",
    "        # Convert predictions to a DataFrame\n",
    "        df_predictions = pd.DataFrame(torch.cat(predictions).cpu().numpy())\n",
    "        \n",
    "        # Save to a csv file\n",
    "        df_predictions.to_csv(f'/prediction/predictions_fold_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
