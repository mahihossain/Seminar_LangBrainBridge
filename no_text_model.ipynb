{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import WhisperModel, Wav2Vec2Processor\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------ Hyperparameters ------- #\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the dimensions of the audio embeddings\n",
    "audio_embedding_dim = 768  # Example, adjust based on your audio feature extractor\n",
    "\n",
    "# Define the number of voxels in the fMRI data for the reading and listening tasks\n",
    "num_voxels_reading = 81133\n",
    "num_voxels_listening = 81133\n",
    "\n",
    "# Define the processor for the audio feature extractor \n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")  # Or your chosen model\n",
    "\n",
    "# Define the directory where your data is stored\n",
    "data_dir = '/path/to/your/data'\n",
    "\n",
    "# Define the filename pattern for your fMRI data files\n",
    "fmri_task_split = f'fmri_{task}_{{split}}.npy'  # Use this as a format string\n",
    "\n",
    "\n",
    "# ------- Dataset & DataLoader ------- #\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, audio_data, voxel_data): \n",
    "        self.audio_data = audio_data\n",
    "        self.voxel_data = voxel_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_data[idx], self.voxel_data[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(data_dir, task):\n",
    "    df = pd.read_hdf(\"data/df_text.hdf\")  # Load the DataFrame\n",
    "\n",
    "    audio_data = []\n",
    "    voxel_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        story_name = row['story_name']\n",
    "        aligned_audio_file = row['aligned_audio_file']\n",
    "\n",
    "        # Audio Input Processing \n",
    "        audio_input = audio_processor(aligned_audio_file, return_tensors='pt') \n",
    "\n",
    "        # Load fMRI Target Data\n",
    "        fmri_filename = eval(f\"fmri_{task}_{split}\") \n",
    "        fmri_file = fmri_filename.format(row['subject'])\n",
    "        with h5py.File(fmri_file, 'r') as f: \n",
    "            target_voxel_data = f[story_name][:]  \n",
    "\n",
    "        audio_data.append(audio_input)\n",
    "        voxel_data.append(target_voxel_data)\n",
    "\n",
    "    dataset = MyDataset(audio_data, voxel_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader \n",
    "\n",
    "# ------- Model Definition ------- #\n",
    "class M2BAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M2BAM, self).__init__()\n",
    "        self.audio_model = WhisperModel.from_pretrained(\"openai/whisper-small\")  # Or your chosen audio model\n",
    "\n",
    "        self.multitask_layer = nn.Sequential(\n",
    "            nn.Linear(audio_embedding_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_voxels_reading),  \n",
    "            nn.Linear(2048, num_voxels_listening)  \n",
    "        )\n",
    "\n",
    "    def forward(self, audio_input):  # Only audio_input now\n",
    "        audio_embeddings = self.audio_model(**audio_input).last_hidden_state[:, 0, :]\n",
    "        predictions = self.multitask_layer(audio_embeddings)\n",
    "\n",
    "        reading_pred, listening_pred = predictions[:, :num_voxels_reading], predictions[:, num_voxels_listening:]\n",
    "        return reading_pred, listening_pred\n",
    "\n",
    "\n",
    "# ------- Training Loop ------- #\n",
    "def train_model(dataloader, task, num_epochs):\n",
    "    model = M2BAM()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (text_batch, voxel_batch) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            reading_output, listening_output = model(text_batch)\n",
    "\n",
    "            loss_reading = criterion(reading_output, voxel_batch[:, :num_voxels_reading]) \n",
    "            loss_listening = criterion(listening_output, voxel_batch[:, num_voxels_listening:])\n",
    "            loss = loss_reading + loss_listening\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0: \n",
    "                logging.info(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}')\n",
    "\n",
    "# ------- Main Execution ------- #\n",
    "if __name__ == \"__main__\":\n",
    "    task = 'reading'  # or 'listening'\n",
    "    dataloader = create_dataloader(data_dir, task)\n",
    "    train_model(dataloader, task, num_epochs) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
